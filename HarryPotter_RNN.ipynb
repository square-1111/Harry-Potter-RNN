{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HarryPotter_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/square-1111/Harry-Potter-RNN/blob/master/HarryPotter_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "KAWZ4P8Lh9nN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating Harry Potter samples using Char-RNN"
      ]
    },
    {
      "metadata": {
        "id": "F1uY_m4Jjv0h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Uploading and Preprocessing Data"
      ]
    },
    {
      "metadata": {
        "id": "z55SEQsfmvBz",
        "colab_type": "code",
        "outputId": "928b4bd8-7d12-435d-f3bd-f88ac58eef51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import Adagrad\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "jWSvFx8Nju58",
        "colab_type": "code",
        "outputId": "d7710305-924f-4a9b-d0d5-e1cc22a4f25a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "file_name = files.upload()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2da8c0d6-9d18-4f4d-a0fa-523a1e924486\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2da8c0d6-9d18-4f4d-a0fa-523a1e924486\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving HP.txt to HP.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "soi3B8B-mMV8",
        "colab_type": "code",
        "outputId": "4999f67e-d5df-4ef6-ac57-2f1eb1d6ec50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_name = list(file_name)[0]\n",
        "file_name_tmp = file_name.strip().replace(\" \", \"_\")\n",
        "\n",
        "if file_name != file_name_tmp:\n",
        "  os.rename(file_name, file_name_tmp)\n",
        "  file_name = file_name_tmp\n",
        "\n",
        "print(\"Filename :\", file_name)\n",
        "\n",
        "HP = open(file_name).read()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filename : HP.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qHPyEZaCqI3j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Basic Information about text file"
      ]
    },
    {
      "metadata": {
        "id": "L4g1spdrqHvQ",
        "colab_type": "code",
        "outputId": "f9a699de-a2bd-4d53-aab3-e4535a5f10c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "num_char = len(HP)\n",
        "print(num_char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "437326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xks1jCidnOOF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Characters to Integer and Integer to Characters for one-hot-encoding"
      ]
    },
    {
      "metadata": {
        "id": "rZ8-hGsCnXP9",
        "colab_type": "code",
        "outputId": "1ae217dc-b15a-4a8b-eff1-488c63cc4e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "characters = string.printable\n",
        "idx_to_char = dict(zip(range(1,len(characters) + 1), characters))\n",
        "char_to_idx = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "print(idx_to_char)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: '0', 2: '1', 3: '2', 4: '3', 5: '4', 6: '5', 7: '6', 8: '7', 9: '8', 10: '9', 11: 'a', 12: 'b', 13: 'c', 14: 'd', 15: 'e', 16: 'f', 17: 'g', 18: 'h', 19: 'i', 20: 'j', 21: 'k', 22: 'l', 23: 'm', 24: 'n', 25: 'o', 26: 'p', 27: 'q', 28: 'r', 29: 's', 30: 't', 31: 'u', 32: 'v', 33: 'w', 34: 'x', 35: 'y', 36: 'z', 37: 'A', 38: 'B', 39: 'C', 40: 'D', 41: 'E', 42: 'F', 43: 'G', 44: 'H', 45: 'I', 46: 'J', 47: 'K', 48: 'L', 49: 'M', 50: 'N', 51: 'O', 52: 'P', 53: 'Q', 54: 'R', 55: 'S', 56: 'T', 57: 'U', 58: 'V', 59: 'W', 60: 'X', 61: 'Y', 62: 'Z', 63: '!', 64: '\"', 65: '#', 66: '$', 67: '%', 68: '&', 69: \"'\", 70: '(', 71: ')', 72: '*', 73: '+', 74: ',', 75: '-', 76: '.', 77: '/', 78: ':', 79: ';', 80: '<', 81: '=', 82: '>', 83: '?', 84: '@', 85: '[', 86: '\\\\', 87: ']', 88: '^', 89: '_', 90: '`', 91: '{', 92: '|', 93: '}', 94: '~', 95: ' ', 96: '\\t', 97: '\\n', 98: '\\r', 99: '\\x0b', 100: '\\x0c'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "25lzMiU28Ypq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting into subsequence\n",
        "$max_{len} $=  maximum length of a sample/sub-sequence  for which RNN will unroll i.e. number of time steps  \n",
        "$sample$ = a list of subsequence each of size $ max_{len}$"
      ]
    },
    {
      "metadata": {
        "id": "zKmdn-Up_qav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "next_char = []\n",
        "max_len = 40\n",
        "step = 10\n",
        "for i in range(0, len(HP) - max_len, step):\n",
        "    sentences.append(HP[i: i + max_len])\n",
        "    next_char.append(HP[i + max_len])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rv2d0Es4AlvT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def character_level_one_hot_encoding(samples, max_len):\n",
        "    inp_vec = np.zeros((len(samples),max_len, max(idx_to_char.keys()) + 1))\n",
        "    out_vec = np.zeros((len(samples), max(idx_to_char.keys()) + 1))\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        for j, character in enumerate(sample):\n",
        "            index = char_to_idx.get(character)\n",
        "            inp_vec[i,j,index] = 1\n",
        "        idx = char_to_idx.get(next_char[i]) \n",
        "        out_vec[i, idx] = 1\n",
        "\n",
        "    return inp_vec, out_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R9Tm3n-q8O-_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inp_vec, out_vec = character_level_one_hot_encoding(sentences, max_len = 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6e6snQtbOfZq",
        "colab_type": "code",
        "outputId": "fde92ee2-a929-4a15-9c10-d73cadcc4e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "inp_vec.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43729, 40, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "yYNK31nmOg6c",
        "colab_type": "code",
        "outputId": "fbe920d7-f14c-4105-c6e0-65d634215cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "out_vec.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43729, 101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "HagKJS2AP7dp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Model"
      ]
    },
    {
      "metadata": {
        "id": "Dm833jJ9QKsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A single LSTM"
      ]
    },
    {
      "metadata": {
        "id": "CGUgTlv6QMrE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_units = 256\n",
        "model = Sequential()\n",
        "model.add( LSTM (num_units, input_shape = (max_len, max(idx_to_char.keys() )+1)))\n",
        "model.add(Dense(max(idx_to_char.keys())+1,activation = 'softmax'))\n",
        "\n",
        "# adagrad = Adagrad(lr=0.002)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k3t3t5WKh9BE",
        "colab_type": "code",
        "outputId": "ad16fc1a-b86b-44bc-fa18-cd34816bc936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (None, 256)               366592    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 101)               25957     \n",
            "=================================================================\n",
            "Total params: 392,549\n",
            "Trainable params: 392,549\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YV_hYGMixzVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Checkpoints \n",
        "To record weights of network everytime the improvement loss is observed.\n",
        "These checkpoints will be used for callback.\n",
        "\n",
        "Format Specifiers are used for naming the file."
      ]
    },
    {
      "metadata": {
        "id": "fjImcJqzyMSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "record_file = 'record-{epoch:02d}-{loss:.4f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(record_file, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hw1_vFri5Rhg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Fitting the model"
      ]
    },
    {
      "metadata": {
        "id": "wada1FWU5MmD",
        "colab_type": "code",
        "outputId": "ca76a34d-a687-422d-bef2-f59d055aa52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3508
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(inp_vec, out_vec, epochs=50, batch_size=256, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "43729/43729 [==============================] - 20s 457us/step - loss: 0.4490\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.00581\n",
            "Epoch 2/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.2631\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00581\n",
            "Epoch 3/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.1653\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.00581\n",
            "Epoch 4/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.1269\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.00581\n",
            "Epoch 5/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0770\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00581\n",
            "Epoch 6/50\n",
            "43729/43729 [==============================] - 20s 457us/step - loss: 0.0568\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00581\n",
            "Epoch 7/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.0380\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00581\n",
            "Epoch 8/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0267\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00581\n",
            "Epoch 9/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0212\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00581\n",
            "Epoch 10/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0164\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00581\n",
            "Epoch 11/50\n",
            "43729/43729 [==============================] - 20s 456us/step - loss: 0.0142\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.00581\n",
            "Epoch 12/50\n",
            "43729/43729 [==============================] - 20s 454us/step - loss: 0.0126\n",
            "\n",
            "Epoch 00012: loss did not improve from 0.00581\n",
            "Epoch 13/50\n",
            "43729/43729 [==============================] - 20s 455us/step - loss: 0.0114\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.00581\n",
            "Epoch 14/50\n",
            "43729/43729 [==============================] - 20s 455us/step - loss: 0.0104\n",
            "\n",
            "Epoch 00014: loss did not improve from 0.00581\n",
            "Epoch 15/50\n",
            "43729/43729 [==============================] - 20s 455us/step - loss: 0.0096\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.00581\n",
            "Epoch 16/50\n",
            "43729/43729 [==============================] - 20s 453us/step - loss: 0.0089\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.00581\n",
            "Epoch 17/50\n",
            "43729/43729 [==============================] - 20s 451us/step - loss: 0.0083\n",
            "\n",
            "Epoch 00017: loss did not improve from 0.00581\n",
            "Epoch 18/50\n",
            "43729/43729 [==============================] - 20s 456us/step - loss: 0.0078\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.00581\n",
            "Epoch 19/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.0073\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.00581\n",
            "Epoch 20/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0072\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.00581\n",
            "Epoch 21/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0065\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.00581\n",
            "Epoch 22/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.0063\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.00581\n",
            "Epoch 23/50\n",
            "43729/43729 [==============================] - 20s 454us/step - loss: 0.0058\n",
            "\n",
            "Epoch 00023: loss improved from 0.00581 to 0.00581, saving model to record-23-0.0058.hdf5\n",
            "Epoch 24/50\n",
            "43729/43729 [==============================] - 20s 449us/step - loss: 0.5430\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.00581\n",
            "Epoch 25/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 1.0296\n",
            "\n",
            "Epoch 00025: loss did not improve from 0.00581\n",
            "Epoch 26/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.4626\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.00581\n",
            "Epoch 27/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.2725\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.00581\n",
            "Epoch 28/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.1557\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.00581\n",
            "Epoch 29/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.1015\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.00581\n",
            "Epoch 30/50\n",
            "43729/43729 [==============================] - 20s 457us/step - loss: 0.0761\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.00581\n",
            "Epoch 31/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0539\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.00581\n",
            "Epoch 32/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0385\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.00581\n",
            "Epoch 33/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.0262\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.00581\n",
            "Epoch 34/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0185\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.00581\n",
            "Epoch 35/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0150\n",
            "\n",
            "Epoch 00035: loss did not improve from 0.00581\n",
            "Epoch 36/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0130\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.00581\n",
            "Epoch 37/50\n",
            "43729/43729 [==============================] - 20s 459us/step - loss: 0.0116\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.00581\n",
            "Epoch 38/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0105\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.00581\n",
            "Epoch 39/50\n",
            "43729/43729 [==============================] - 20s 456us/step - loss: 0.0096\n",
            "\n",
            "Epoch 00039: loss did not improve from 0.00581\n",
            "Epoch 40/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0088\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.00581\n",
            "Epoch 41/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0082\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.00581\n",
            "Epoch 42/50\n",
            "43729/43729 [==============================] - 20s 457us/step - loss: 0.0077\n",
            "\n",
            "Epoch 00042: loss did not improve from 0.00581\n",
            "Epoch 43/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0072\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.00581\n",
            "Epoch 44/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.0067\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.00581\n",
            "Epoch 45/50\n",
            "43729/43729 [==============================] - 20s 452us/step - loss: 0.0064\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.00581\n",
            "Epoch 46/50\n",
            "43729/43729 [==============================] - 20s 450us/step - loss: 0.0198\n",
            "\n",
            "Epoch 00046: loss did not improve from 0.00581\n",
            "Epoch 47/50\n",
            "43729/43729 [==============================] - 20s 452us/step - loss: 1.1653\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.00581\n",
            "Epoch 48/50\n",
            "43729/43729 [==============================] - 20s 460us/step - loss: 0.4798\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.00581\n",
            "Epoch 49/50\n",
            "43729/43729 [==============================] - 20s 458us/step - loss: 0.2480\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.00581\n",
            "Epoch 50/50\n",
            "43729/43729 [==============================] - 20s 457us/step - loss: 0.1489\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.00581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff34f14c9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "HVZ6h7zw9gHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Prediction"
      ]
    },
    {
      "metadata": {
        "id": "QZ0Mk2fz9fnv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "weight_file = \"record-10-0.0121.hdf5\"\n",
        "model.load_weights(weight_file)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vfMFVj8flIJl",
        "colab_type": "code",
        "outputId": "1d88f740-28f2-4b23-e1bc-524148cc028f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "generate_next_of = input('Input a sequence of characters ')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input a sequence of characters Hagrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lQHhghb5bQFG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### One-hot Encoding"
      ]
    },
    {
      "metadata": {
        "id": "N0QiZGzimYkU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def padding(sequence):\n",
        "  if len(sequence) > max_len:\n",
        "    out = sequence[len(sequence) - max_len :len(sequence)]\n",
        "  else:\n",
        "    out = (max_len - len(sequence))*' ' + sequence\n",
        "  return [out]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sMK3Pjvzya7h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "next_of = padding(generate_next_of)\n",
        "# inp_test_vec, rand = character_level_one_hot_encoding(next_of, max_len)\n",
        "# inp_test_vec.shape\n",
        "# len(generate_next_of)\n",
        "# print(next_of)\n",
        "# print(next_of[0][1:len(next_of[0])])\n",
        "# len(next_of[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xP0lYVadvQ5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "output = \"\"\n",
        "for i in range(1000):\n",
        "#     print(next_of)\n",
        "    inp_test_vec, rand = character_level_one_hot_encoding(next_of, max_len)\n",
        "#     print(inp_test_vec.shape)\n",
        "    prediction = model.predict(inp_test_vec)\n",
        "#     print(prediction)\n",
        "    index = np.argmax(prediction[0])\n",
        "#     print(index)\n",
        "    pred_char = idx_to_char.get(index)\n",
        "    output = output + pred_char\n",
        "    next_of[0] = next_of[0] + pred_char\n",
        "#     print(next_of)\n",
        "    next_of[0] = next_of[0][1:len(next_of[0])]\n",
        "#     print(len(next_of[0]))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9JQOj2d7eme",
        "colab_type": "code",
        "outputId": "4d73ae85-ac45-4615-e56c-26a5c3bc3f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'live he dand.\\n\"We han, be, I\\'ve see...\" he day, Pookem, but was going to foll out oft the migrt siven edgalon. I not be nowss ffom the foo, whie daye to the\\npeed at oll over his face, squitched, be caugh nis coust the cumbed it was a bidle sookeres, hear. It wand to as ining the cambres out af it whoke erter, nut he culled foo stay\\nhe dots. He coulde\\'t scecherid;\\nThe mall sarry cloake they were back on\\nright. It was a once, browall whiet the lettred.\\n\"Whould they lorked doon. His wen id, pookin lookin Coursec- at the candy that the catch the bake back to the\\npeet sole it wass. \"If loow, sav, Potcen,\" said Hermione. \"Weverese\\'s is aree secudly sermessird.\\n\"She leamed Winn mols in his nicher of them.\\nShe looked students frimwown do, what he was looking out of mingaring behand the night clean, betires, he gas sock of the Dumbledore wasts, pinning the sook\\nwas cound he. I could hear donger, Ha\\nron\\'s crusings, as the goll send Harry fell thourd see him.... do neverass sponee surd in the stu'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}